{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb86ab89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-i\", \"https://pypi.org/simple\", \"--no-deps\", \"transformers==4.46.2\"])\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-i\", \"https://pypi.org/simple\",\n",
    "                       \"datasets==3.0.1\", \"pyarrow\", \"captum==0.7.0\",\n",
    "                       \"scipy==1.14.1\", \"seaborn==0.13.2\", \"scikit-learn==1.5.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29ab76cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: pip --upgrade\n",
      "Installing: regex==2024.11.6 huggingface-hub==0.26.2 safetensors==0.4.5 requests==2.32.3 tqdm==4.66.5 pyyaml==6.0.2 packaging==25.0 numpy==2.1.3\n",
      "Installing: transformers==4.46.2\n",
      "Installing: datasets==3.0.1 pyarrow\n",
      "Installing: captum==0.7.0 scipy==1.14.1 seaborn==0.13.2 scikit-learn==1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs, index=\"https://pypi.org/simple\"):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-i\", index] + pkgs\n",
    "    print(\"Installing:\", \" \".join(pkgs))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# 升级 pip\n",
    "pip_install([\"pip\", \"--upgrade\"])\n",
    "\n",
    "# transformers 所需的关键依赖（避免 tokenizers 源码编译）\n",
    "pip_install([\n",
    "    \"regex==2024.11.6\",\n",
    "    \"huggingface-hub==0.26.2\",\n",
    "    \"safetensors==0.4.5\",\n",
    "    \"requests==2.32.3\",\n",
    "    \"tqdm==4.66.5\",\n",
    "    \"pyyaml==6.0.2\",\n",
    "    \"packaging==25.0\",\n",
    "    \"numpy==2.1.3\"\n",
    "])\n",
    "\n",
    "# 安装 transformers（已在代码中使用 use_fast=False，不依赖 tokenizers）\n",
    "pip_install([\"transformers==4.46.2\"])\n",
    "\n",
    "# 其余依赖（datasets 及其需要的 pyarrow、以及分析用包）\n",
    "pip_install([\n",
    "    \"datasets==3.0.1\",\n",
    "    \"pyarrow\"\n",
    "])\n",
    "pip_install([\n",
    "    \"captum==0.7.0\",\n",
    "    \"scipy==1.14.1\",\n",
    "    \"seaborn==0.13.2\",\n",
    "    \"scikit-learn==1.5.2\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed3e5626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel exe: c:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\.conda\\python.exe\n",
      "Installing: regex==2024.11.6 numpy==2.1.3 packaging==25.0 pyyaml==6.0.2 requests==2.32.3 tqdm==4.66.5 huggingface-hub==0.26.2 safetensors==0.4.5\n",
      "Installing: transformers==4.46.2\n",
      "Installing: datasets==3.0.1 pyarrow\n",
      "Installing: captum==0.7.0 scipy==1.14.1 seaborn==0.13.2 scikit-learn==1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs, index=\"https://pypi.org/simple\"):\n",
    "    print(\"Installing:\", \" \".join(pkgs))\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-i\", index] + pkgs)\n",
    "\n",
    "print(\"Kernel exe:\", sys.executable)\n",
    "\n",
    "# 关键依赖修复（regex 缺失会导致 transformers 版本检查失败）\n",
    "pip_install([\"regex==2024.11.6\", \"numpy==2.1.3\", \"packaging==25.0\", \"pyyaml==6.0.2\",\n",
    "             \"requests==2.32.3\", \"tqdm==4.66.5\", \"huggingface-hub==0.26.2\", \"safetensors==0.4.5\"])\n",
    "\n",
    "# 安装 transformers（不拉 tokenizers 以避免编译）\n",
    "pip_install([\"transformers==4.46.2\"])\n",
    "\n",
    "# 其余依赖（datasets 及其 pyarrow、分析用包）\n",
    "pip_install([\"datasets==3.0.1\", \"pyarrow\"])\n",
    "pip_install([\"captum==0.7.0\", \"scipy==1.14.1\", \"seaborn==0.13.2\", \"scikit-learn==1.5.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c665974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: protobuf==5.28.3 to c:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\.conda\\python.exe\n",
      "protobuf backend: python\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, os\n",
    "\n",
    "def pip_install(pkgs, index=\"https://pypi.org/simple\"):\n",
    "    print(\"Installing:\", \" \".join(pkgs), \"to\", sys.executable)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-i\", index] + pkgs)\n",
    "\n",
    "# 1) 安装 protobuf（5.x 对 Python 3.13 兼容）\n",
    "pip_install([\"protobuf==5.28.3\"])\n",
    "\n",
    "# 2) 可选：强制使用纯 Python 后端（避免本地扩展问题）\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# 3) 验证 protobuf 后端（upb 或 python 都可用）\n",
    "from google.protobuf.internal import api_implementation\n",
    "print(\"protobuf backend:\", api_implementation.Type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3dd2bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel exe: c:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\.conda\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess, os\n",
    "\n",
    "def pip_install(pkgs): subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-i\", \"https://pypi.org/simple\"] + pkgs)\n",
    "\n",
    "pip_install([\"protobuf==5.28.3\",\"regex==2024.11.6\",\"huggingface-hub==0.26.2\",\"safetensors==0.4.5\",\"requests==2.32.3\",\"tqdm==4.66.5\",\"pyyaml==6.0.2\",\"packaging==25.0\",\"numpy==2.1.3\"])\n",
    "pip_install([\"transformers==4.46.2\"])\n",
    "pip_install([\"datasets==3.0.1\",\"pyarrow\"])\n",
    "pip_install([\"captum==0.7.0\",\"scipy==1.14.1\",\"seaborn==0.13.2\",\"scikit-learn==1.5.2\"])\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "print(\"kernel exe:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e44e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65648dca1ac4c059c1add95d7755591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\hf_cache\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers model/tokenizer OK\n"
     ]
    }
   ],
   "source": [
    "import transformers, datasets, captum, scipy, sklearn, seaborn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "print(\"Imports OK\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    use_fast=False,       # 已在代码中使用慢速分词器，避免 tokenizers 依赖\n",
    "    cache_dir=\"./hf_cache\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=1,\n",
    "    cache_dir=\"./hf_cache\"\n",
    ")\n",
    "print(\"Transformers model/tokenizer OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16193411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project running on: cpu\n",
      "Data Loaded: (50000, 2)\n",
      "--- Starting Training ---\n",
      "Epoch 1 | Base Loss: 0.4596 Acc: 0.7746 | Attn Loss: 0.4437 Acc: 0.7869 | BiLSTM-Mean Loss: 0.4659 Acc: 0.7714 | CNN-Text Loss: 0.5702 Acc: 0.6969 | CoAttn Loss: 0.4365 Acc: 0.7885\n",
      "Epoch 2 | Base Loss: 0.2954 Acc: 0.8768 | Attn Loss: 0.2722 Acc: 0.8874 | BiLSTM-Mean Loss: 0.2937 Acc: 0.8773 | CNN-Text Loss: 0.4373 Acc: 0.7933 | CoAttn Loss: 0.2713 Acc: 0.8877\n",
      "Epoch 3 | Base Loss: 0.2323 Acc: 0.9086 | Attn Loss: 0.2065 Acc: 0.9188 | BiLSTM-Mean Loss: 0.2244 Acc: 0.9118 | CNN-Text Loss: 0.3603 Acc: 0.8395 | CoAttn Loss: 0.1994 Acc: 0.9215\n",
      "Epoch 4 | Base Loss: 0.1820 Acc: 0.9303 | Attn Loss: 0.1506 Acc: 0.9445 | BiLSTM-Mean Loss: 0.1762 Acc: 0.9327 | CNN-Text Loss: 0.3027 Acc: 0.8686 | CoAttn Loss: 0.1406 Acc: 0.9471\n",
      "Epoch 5 | Base Loss: 0.1391 Acc: 0.9490 | Attn Loss: 0.0997 Acc: 0.9648 | BiLSTM-Mean Loss: 0.1310 Acc: 0.9518 | CNN-Text Loss: 0.2569 Acc: 0.8944 | CoAttn Loss: 0.0846 Acc: 0.9703\n",
      "BiLSTM-Mean : Accuracy = 0.8812, F1-Score = 0.8821\n",
      "CNN-Text    : Accuracy = 0.8762, F1-Score = 0.8787\n",
      "Co-Attn     : Accuracy = 0.8821, F1-Score = 0.8827\n",
      "\n",
      "--- Final Results (Test Set) ---\n",
      "Baseline (Bi-LSTM)     : Accuracy = 0.8775, F1-Score = 0.8775\n",
      "Proposed (Attn-BiLSTM) : Accuracy = 0.8808, F1-Score = 0.8814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f45018a4374dc3a0080d8270b2ca3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\hf_cache\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76a1b2cadb44ef19cd505c4eb8db08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb5489449d24b1da7abc91a77e13b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d8797c912d41deafbd75bc33b321c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2326fe6fcf594e438f5bfb4918844ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4f9a1eadb9468bbe4c677cca3b32df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4f3c2d2a0b4b3ca6a6f909555a1db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Henry Zhang\\Desktop\\基于双向 LSTM 与自注意力机制的可解释性情感分析研究\\hf_cache\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e915ef8a38446ea8cf957fc52a05bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd19bf2ba1ef4724b82f4bf70399df70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db945053603145e994cb20a4dac7affa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c4b3fedee147c2840fbfb3facff44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-base  : Acc=0.5262, F1=0.6309, Params=109.5M, TrainTime=255.4s, Throughput=6.3/s\n",
      "RoBERTa-b  : Acc=0.5055, F1=0.6686, Params=124.6M, TrainTime=255.2s, Throughput=6.3/s\n",
      "DistilBERT : Acc=0.6776, F1=0.5424, Params=67.0M, TrainTime=228.6s, Throughput=14.0/s\n",
      "Ablation LSTM-only : Acc = 0.8784, F1 = 0.8769\n",
      "Ablation CNN-only  : Acc = 0.8716, F1 = 0.8740\n",
      "Ablation Fusion    : Acc = 0.8751, F1 = 0.8719\n",
      "\n",
      "--- Interpretability Visualization ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn-BiLSTM F1 mean=0.8693 std=0.0104 95%CI=[0.8587,0.8751] vs Baseline mean=0.8719 std=0.0042 95%CI=[0.8684,0.8755] paired_t p=0.6893\n",
      "ECE(calibrated)=0.4665\n"
     ]
    }
   ],
   "source": [
    "\"\"\"实验主脚本：包含数据预处理、模型定义（BiLSTM、Attn‑BiLSTM、CNN‑Text、Co‑Attn）、\n",
    "训练与评估、可解释性（注意力/梯度）与校准分析（温度缩放/ECE）、以及 Transformer 快速基线对比。\n",
    "\n",
    "主要入口位于 __main__：先训练并评估 RNN/CNN/Co‑Attn 系列，再进行阈值选择与测试集汇报；\n",
    "随后运行 Transformer 基线（BERT/RoBERTa/DistilBERT）的快速对比与资源统计；最后生成可视化与统计结果。\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from collections import Counter\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import random\n",
    "import os\n",
    "from captum.attr import IntegratedGradients, Saliency\n",
    "from scipy.stats import ttest_rel\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Project running on: {device}\")\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('IMDB Dataset.csv')\n",
    "    print(f\"Data Loaded: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: IMDB Dataset.csv not found.\")\n",
    "    df = pd.DataFrame({'review': ['good movie']*100 + ['bad movie']*100,\n",
    "                       'sentiment': ['positive']*100 + ['negative']*100})\n",
    "\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "\n",
    "all_text = ' '.join(df['cleaned_review'].values)\n",
    "words = all_text.split()\n",
    "word_counts = Counter(words)\n",
    "\n",
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "common_words = word_counts.most_common(MAX_VOCAB_SIZE)\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(common_words)}\n",
    "vocab['<UNK>'] = 1\n",
    "vocab['<PAD>'] = 0\n",
    "\n",
    "\n",
    "MAX_LEN = 200\n",
    "\n",
    "\n",
    "def text_to_indices(text, vocab, max_len):\n",
    "    words = text.split()\n",
    "    indices = [vocab.get(w, vocab['<UNK>']) for w in words]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len):\n",
    "        \"\"\"IMDB 文本数据集封装\n",
    "\n",
    "        将清洗后的文本映射为固定长度的索引序列（右侧填充，超长截断），\n",
    "        同时保留二分类标签；供非 Transformer 模型使用。\n",
    "        \"\"\"\n",
    "        self.labels = torch.tensor(df['label'].values, dtype=torch.float32)\n",
    "        self.reviews = [text_to_indices(text, vocab, max_len) for text in df['cleaned_review']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "train_df2, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "train_dataset = IMDBDataset(train_df, vocab, MAX_LEN)\n",
    "test_dataset = IMDBDataset(test_df, vocab, MAX_LEN)\n",
    "val_dataset = IMDBDataset(val_df, vocab, MAX_LEN)\n",
    "\n",
    "#HuggingFace 模型数据集与训练评估函数\n",
    "class HFTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        \"\"\"面向 HuggingFace Transformer 的数据集\n",
    "\n",
    "        使用分词器进行 `padding='max_length'` 与 `truncation=True` 编码，\n",
    "        返回字典形式的输入与对应标签，以便 `AutoModelForSequenceClassification` 前向。\n",
    "        \"\"\"\n",
    "        self.enc = tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.enc.items()}, self.labels[idx]\n",
    "\n",
    "def train_transformer(model_name, train_df, val_df, max_len=256, epochs=3, batch_size=16, lr=2e-5, limit_train=None, limit_val=None, max_train_batches=None, freeze_backbone=None):\n",
    "    \"\"\"微调并验证 Transformer 基线\n",
    "\n",
    "    参数：\n",
    "    - model_name：预训练模型名称（如 `bert-base-uncased`）\n",
    "    - train_df/val_df：包含 `review` 与 `label` 的数据框\n",
    "    - max_len/epochs/batch_size/lr：文本长度与训练超参数\n",
    "    - limit_train/limit_val/max_train_batches：在 CPU 上限制样本/批次数以加速调试\n",
    "    - freeze_backbone：是否冻结主干参数（CPU 场景默认冻结）\n",
    "\n",
    "    返回：模型、分词器、验证集 `acc/f1`、参数量、训练时长与吞吐（样本/秒）。\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, cache_dir=\"./hf_cache\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1, cache_dir=\"./hf_cache\")\n",
    "    model.to(device)\n",
    "    if limit_train is not None:\n",
    "        train_df = train_df.iloc[:int(limit_train)]\n",
    "    if limit_val is not None:\n",
    "        val_df = val_df.iloc[:int(limit_val)]\n",
    "    train_ds = HFTextDataset(train_df['review'].tolist(), train_df['label'].values.tolist(), tokenizer, max_len)\n",
    "    val_ds = HFTextDataset(val_df['review'].tolist(), val_df['label'].values.tolist(), tokenizer, max_len)\n",
    "    train_loader_hf = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_hf = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    if freeze_backbone is None:\n",
    "        freeze_backbone = True if device.type == 'cpu' else False\n",
    "    if freeze_backbone:\n",
    "        base = getattr(model, model.base_model_prefix, None)\n",
    "        if base is not None:\n",
    "            for p in base.parameters():\n",
    "                p.requires_grad = False\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    n_samples = 0\n",
    "    t0 = time.time()\n",
    "    if device.type == 'cpu' and epochs > 1:\n",
    "        epochs = 1\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        bcount = 0\n",
    "        for batch, labels in train_loader_hf:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_samples += labels.size(0)\n",
    "            bcount += 1\n",
    "            if max_train_batches is not None and bcount >= max_train_batches:\n",
    "                break\n",
    "    t_train = time.time() - t0\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in val_loader_hf:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch).logits\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    throughput = n_samples / t_train if t_train > 0 else 0.0\n",
    "    return model, tokenizer, acc, f1, param_count, t_train, throughput\n",
    "\n",
    "def evaluate_transformer(model, tokenizer, df, max_len=256):\n",
    "    \"\"\"在给定数据上评估已微调的 Transformer 模型，返回 `acc, f1`。\"\"\"\n",
    "    ds = tokenizer(df['review'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')\n",
    "    labels = torch.tensor(df['label'].values.tolist(), dtype=torch.float32)\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(labels), 32):\n",
    "            batch = {k: v[i:i+32].to(device) for k, v in ds.items()}\n",
    "            logits = model(**batch).logits\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "    acc = accuracy_score(labels.numpy(), np.array(all_preds))\n",
    "    f1 = f1_score(labels.numpy(), np.array(all_preds))\n",
    "    return acc, f1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class BiLSTM_Baseline(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64):\n",
    "        \"\"\"BiLSTM 基线模型\n",
    "\n",
    "        结构：嵌入 → 双向 LSTM → 时间维均值池化 → 全连接 → Logits。\n",
    "        用于二分类，配合 `BCEWithLogitsLoss` 与 Sigmoid 后阈值判断。\n",
    "        \"\"\"\n",
    "        super(BiLSTM_Baseline, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = torch.mean(lstm_out, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BiLSTM_Baseline_Mean(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64):\n",
    "        \"\"\"BiLSTM‑Mean 更强无注意力基线\n",
    "\n",
    "        在双向 LSTM 输出上做全局均值池化，以提升对长文本的时间聚合稳健性。\n",
    "        \"\"\"\n",
    "        super(BiLSTM_Baseline_Mean, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = torch.mean(lstm_out, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64):\n",
    "        \"\"\"Attn‑BiLSTM 自注意力模型\n",
    "\n",
    "        在双向 LSTM 的时间序列输出上计算注意力权重 `alpha`，经 Softmax 归一后加权求和得到上下文向量，\n",
    "        同时返回词级权重以供可视化与忠实度评估。\n",
    "        \"\"\"\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention_weights_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        attn_weights = self.attention_weights_layer(lstm_out)\n",
    "        alpha = torch.softmax(attn_weights, dim=1)\n",
    "        context = torch.bmm(alpha.transpose(1, 2), lstm_out).squeeze(1)\n",
    "        out = self.fc(context)\n",
    "        return out, alpha\n",
    "\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_filters=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        \"\"\"CNN‑Text 文本卷积模型\n",
    "\n",
    "        多尺度一维卷积提取 n‑gram 特征，时间维最大池化后拼接，接全连接得到二分类 logits。\n",
    "        \"\"\"\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, num_filters, k, padding=k//2)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), 1)\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        x_emb = embeds.transpose(1, 2)\n",
    "        conv_outs = [torch.relu(conv(x_emb)) for conv in self.convs]\n",
    "        pooled = [torch.max(c, dim=2).values for c in conv_outs]\n",
    "        out = torch.cat(pooled, dim=1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CoAttention_CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64, num_filters=64, kernel_sizes=(3, 4, 5), dropout=0.3):\n",
    "        \"\"\"并行融合注意力（Co‑Attn）\n",
    "\n",
    "        将 CNN 与 BiLSTM 特征在时间维对齐后拼接，对融合序列施加注意力并聚合；\n",
    "        兼顾局部短语模式与长程依赖的证据聚合。\n",
    "        \"\"\"\n",
    "        super(CoAttention_CNN_BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, num_filters, k, padding=k//2)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        fused_dim = num_filters * len(kernel_sizes) + hidden_dim * 2\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.fc = nn.Linear(fused_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        x_emb = embeds.transpose(1, 2)\n",
    "        conv_outs = [torch.relu(conv(x_emb)) for conv in self.convs]\n",
    "        seq_len = embeds.size(1)\n",
    "        conv_aligned = []\n",
    "        for c in conv_outs:\n",
    "            if c.size(2) > seq_len:\n",
    "                c = c[:, :, :seq_len]\n",
    "            elif c.size(2) < seq_len:\n",
    "                pad_len = seq_len - c.size(2)\n",
    "                c = torch.nn.functional.pad(c, (0, pad_len))\n",
    "            conv_aligned.append(c)\n",
    "        cnn_feat = torch.cat(conv_aligned, dim=1).transpose(1, 2)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        fused = torch.cat([cnn_feat, lstm_out], dim=2)\n",
    "        scores = self.attn(fused)\n",
    "        alpha = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(alpha.transpose(1, 2), fused).squeeze(1)\n",
    "        context = self.dropout(context)\n",
    "        out = self.fc(context)\n",
    "        return out, alpha\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, model_name=\"Model\", threshold=0.5):\n",
    "    \"\"\"单轮训练与即时准确率统计\n",
    "\n",
    "    对 `Baseline/Attention` 两类模型分别兼容（Attention 返回 `(logits, alpha)`），\n",
    "    以 Sigmoid 后阈值进行批次内准确率估计。\n",
    "    返回：平均损失与准确率。\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        if model_name == \"Attention\":\n",
    "            outputs, _ = model(inputs)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        predicted = (probs > threshold).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(train_loader), correct / total\n",
    "\n",
    "def evaluate_model(model, test_loader, model_name=\"Model\", threshold=0.5):\n",
    "    \"\"\"在给定数据加载器上评估模型，返回 `acc, f1`。\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            if model_name == \"Attention\":\n",
    "                outputs, _ = model(inputs)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predicted = (probs > threshold).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, f1\n",
    "\n",
    "def visualize_attention(text, model, vocab, max_len):\n",
    "    model.eval()\n",
    "    cleaned = clean_text(text)\n",
    "    indices = text_to_indices(cleaned, vocab, max_len)\n",
    "    input_tensor = indices.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, alpha = model(input_tensor)\n",
    "        prediction = output.item()\n",
    "\n",
    "    valid_len = len(cleaned.split())\n",
    "    valid_len = min(valid_len, max_len)\n",
    "\n",
    "    weights = alpha.squeeze().cpu().numpy()[:valid_len]\n",
    "    words = cleaned.split()[:valid_len]\n",
    "\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    sns.heatmap([weights], xticklabels=words, yticklabels=['Attention'], cmap='Reds', cbar=True)\n",
    "    plt.title(f\"Prediction: {'Positive' if prediction > 0.5 else 'Negative'} ({prediction:.2f})\")\n",
    "    plt.show()\n",
    "\n",
    "def find_best_threshold(model, loader, name=\"Model\"):\n",
    "    \"\"\"在验证集上网格搜索决策阈值（0.1–0.9），以最大化 F1。\"\"\"\n",
    "    model.eval()\n",
    "    thresholds = torch.linspace(0.1, 0.9, steps=17).tolist()\n",
    "    best_t = 0.5\n",
    "    best_f1 = -1.0\n",
    "    with torch.no_grad():\n",
    "        for t in thresholds:\n",
    "            preds = []\n",
    "            labels = []\n",
    "            for inputs, y in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                if name == \"Attention\":\n",
    "                    out, _ = model(inputs)\n",
    "                else:\n",
    "                    out = model(inputs)\n",
    "                probs = torch.sigmoid(out)\n",
    "                preds.extend((probs > t).float().cpu().numpy())\n",
    "                labels.extend(y.cpu().numpy())\n",
    "            f1 = f1_score(labels, preds)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_t = t\n",
    "    return float(best_t)\n",
    "\n",
    "def predict_logits(model, loader, name):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, y in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            if name == \"Attention\":\n",
    "                out, _ = model(inputs)\n",
    "            else:\n",
    "                out = model(inputs)\n",
    "            p = torch.sigmoid(out)\n",
    "            preds.extend(p.cpu().numpy())\n",
    "            labels.extend(y.cpu().numpy())\n",
    "    return np.array(preds).reshape(-1), np.array(labels).reshape(-1)\n",
    "\n",
    "\n",
    "def visualize_attention(text, model, vocab, max_len, save_path=None):\n",
    "    \"\"\"生成词级注意力热力图\n",
    "\n",
    "    输入原始文本，经过清洗与索引化后前向，基于返回的 `alpha` 绘制热力图；\n",
    "    若 `save_path` 提供则保存到文件，否则直接显示。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cleaned = clean_text(text)\n",
    "    indices = text_to_indices(cleaned, vocab, max_len)\n",
    "    input_tensor = indices.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output, alpha = model(input_tensor)\n",
    "        prediction = torch.sigmoid(output).item()\n",
    "    valid_len = min(len(cleaned.split()), max_len)\n",
    "    weights = alpha.squeeze().cpu().numpy()[:valid_len]\n",
    "    words = cleaned.split()[:valid_len]\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    sns.heatmap([weights], xticklabels=words, yticklabels=['Attention'], cmap='Reds', cbar=True)\n",
    "    plt.title(f\"Prediction: {'Positive' if prediction > 0.5 else 'Negative'} ({prediction:.2f})\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=200)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def generate_attention_figures(model, vocab, max_len):\n",
    "    \"\"\"批量生成若干示例的注意力热力图，用于论文配图。\"\"\"\n",
    "    os.makedirs('figs', exist_ok=True)\n",
    "    t1 = \"nothing bright technological comingofage thoughtfully deeply relatable clear story central performances understated beauty captivating growth opportunities\"\n",
    "    t2 = \"This movie was absolutely terrible and a waste of time but the acting was okay\"\n",
    "    t3 = \"i loved the story it was amazing and touching\"\n",
    "    visualize_attention(t1, model, vocab, max_len, os.path.join('figs', 'attn_long_positive.png'))\n",
    "    visualize_attention(t2, model, vocab, max_len, os.path.join('figs', 'attn_negative.png'))\n",
    "    visualize_attention(t3, model, vocab, max_len, os.path.join('figs', 'attn_positive.png'))\n",
    "\n",
    "def predict_on_texts(model, texts, name, max_len, vocab, batch_size=128):\n",
    "    \"\"\"批量预测文本的正类概率（Sigmoid），兼容 Attention 与 Baseline。\"\"\"\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_idx = [text_to_indices(clean_text(t), vocab, max_len) for t in batch]\n",
    "            inputs = torch.stack(batch_idx).to(device)\n",
    "            if name == \"Attention\":\n",
    "                out, _ = model(inputs)\n",
    "            else:\n",
    "                out = model(inputs)\n",
    "            p = torch.sigmoid(out).squeeze(1).cpu().numpy().tolist()\n",
    "            probs.extend(p)\n",
    "    return np.array(probs)\n",
    "\n",
    "def save_dataset_curves(model, df, name, vocab, max_len):\n",
    "    \"\"\"基于预测概率绘制并保存 ROC、PR 与混淆矩阵曲线/图。\"\"\"\n",
    "    texts = df['cleaned_review'].tolist()\n",
    "    labels = df['label'].values.astype(int)\n",
    "    probs = predict_on_texts(model, texts, name, max_len, vocab)\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    precision, recall, _ = precision_recall_curve(labels, probs)\n",
    "    os.makedirs('figs', exist_ok=True)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'AUC={roc_auc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.legend()\n",
    "    plt.title('ROC')\n",
    "    plt.savefig(os.path.join('figs', 'roc_attn.png'), bbox_inches='tight', dpi=200)\n",
    "    plt.close()\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('PR')\n",
    "    plt.savefig(os.path.join('figs', 'pr_attn.png'), bbox_inches='tight', dpi=200)\n",
    "    plt.close()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join('figs', 'cm_attn.png'), bbox_inches='tight', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def save_threshold_sweep(model, df, name, vocab, max_len):\n",
    "    \"\"\"绘制 F1‑阈值曲线，辅助在验证集上选择最优阈值。\"\"\"\n",
    "    texts = df['cleaned_review'].tolist()\n",
    "    labels = df['label'].values.astype(int)\n",
    "    probs = predict_on_texts(model, texts, name, max_len, vocab)\n",
    "    ts = np.linspace(0.05, 0.95, 19)\n",
    "    f1s = [f1_score(labels, (probs > t).astype(int)) for t in ts]\n",
    "    os.makedirs('figs', exist_ok=True)\n",
    "    plt.figure()\n",
    "    plt.plot(ts, f1s, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1')\n",
    "    plt.title('F1 vs Threshold')\n",
    "    plt.savefig(os.path.join('figs', 'f1_vs_threshold.png'), bbox_inches='tight', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def save_attention_from_dataset(model, df, vocab, max_len, n_pos=3, n_neg=3):\n",
    "    \"\"\"从数据集中挑选高置信正/负样本，保存词级注意力热力图。\"\"\"\n",
    "    texts = df['cleaned_review'].tolist()\n",
    "    labels = df['label'].values.astype(int)\n",
    "    probs = predict_on_texts(model, texts, \"Attention\", max_len, vocab)\n",
    "    pos_idx = np.where(labels == 1)[0]\n",
    "    neg_idx = np.where(labels == 0)[0]\n",
    "    pos_sorted = pos_idx[np.argsort(probs[pos_idx])[::-1][:n_pos]]\n",
    "    neg_sorted = neg_idx[np.argsort(probs[neg_idx])[:n_neg]]\n",
    "    os.makedirs('figs', exist_ok=True)\n",
    "    for i, idx in enumerate(pos_sorted, 1):\n",
    "        visualize_attention(texts[idx], model, vocab, max_len, os.path.join('figs', f'attn_dataset_pos_{i}.png'))\n",
    "    for i, idx in enumerate(neg_sorted, 1):\n",
    "        visualize_attention(texts[idx], model, vocab, max_len, os.path.join('figs', f'attn_dataset_neg_{i}.png'))\n",
    "\n",
    "def save_length_bucket_examples(model, df, vocab, max_len):\n",
    "    \"\"\"按长度分层（≤50、≥150）抽取样例并保存注意力热力图。\"\"\"\n",
    "    df2 = df.copy()\n",
    "    df2['len'] = df2['cleaned_review'].apply(lambda t: len(t.split()))\n",
    "    short = df2[df2['len'] <= 50]\n",
    "    long = df2[df2['len'] >= 150]\n",
    "    texts_short_pos = short[short['label'] == 1]['cleaned_review'].head(2).tolist()\n",
    "    texts_short_neg = short[short['label'] == 0]['cleaned_review'].head(2).tolist()\n",
    "    texts_long_pos = long[long['label'] == 1]['cleaned_review'].head(2).tolist()\n",
    "    texts_long_neg = long[long['label'] == 0]['cleaned_review'].head(2).tolist()\n",
    "    os.makedirs('figs', exist_ok=True)\n",
    "    for i, t in enumerate(texts_short_pos, 1):\n",
    "        visualize_attention(t, model, vocab, max_len, os.path.join('figs', f'attn_short_pos_{i}.png'))\n",
    "    for i, t in enumerate(texts_short_neg, 1):\n",
    "        visualize_attention(t, model, vocab, max_len, os.path.join('figs', f'attn_short_neg_{i}.png'))\n",
    "    for i, t in enumerate(texts_long_pos, 1):\n",
    "        visualize_attention(t, model, vocab, max_len, os.path.join('figs', f'attn_long_pos_{i}.png'))\n",
    "    for i, t in enumerate(texts_long_neg, 1):\n",
    "        visualize_attention(t, model, vocab, max_len, os.path.join('figs', f'attn_long_neg_{i}.png'))\n",
    "\n",
    "def lr_grid_search(model_builder, name, train_loader, val_loader, lrs=(1e-3, 5e-4, 1e-4), epochs=2):\n",
    "    \"\"\"简易学习率搜索：在小轮次上比较验证集 F1，返回最优学习率与对应 F1。\"\"\"\n",
    "    best_lr = None\n",
    "    best_f1 = -1.0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for lr in lrs:\n",
    "        model = model_builder().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        for _ in range(epochs):\n",
    "            train_model(model, train_loader, optimizer, criterion, name, 0.5)\n",
    "        _, f1 = evaluate_model(model, val_loader, name, 0.5)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_lr = lr\n",
    "    return best_lr, best_f1\n",
    "\n",
    "def bootstrap_ci(values, n_boot=1000, alpha=0.05):\n",
    "    \"\"\"Bootstrap 置信区间：返回均值、标准差与 95% 置信区间上下界。\"\"\"\n",
    "    vals = np.array(values)\n",
    "    boot = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.choice(vals, size=len(vals), replace=True)\n",
    "        boot.append(np.mean(sample))\n",
    "    boot = np.array(boot)\n",
    "    lower = np.quantile(boot, alpha/2)\n",
    "    upper = np.quantile(boot, 1-alpha/2)\n",
    "    return float(np.mean(vals)), float(np.std(vals)), float(lower), float(upper)\n",
    "\n",
    "def run_multi_seed(model_builder, name, train_loader, test_loader, seeds=(42,43,44,45,46)):\n",
    "    \"\"\"多随机种子复现：返回各种子下的 Acc/F1 列表，用于统计聚合与显著性检验。\"\"\"\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for s in seeds:\n",
    "        set_seed(s)\n",
    "        model = model_builder().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        for _ in range(5):\n",
    "            train_model(model, train_loader, optimizer, criterion, name, 0.5)\n",
    "        acc, f1 = evaluate_model(model, test_loader, name, 0.5)\n",
    "        accs.append(acc); f1s.append(f1)\n",
    "    return accs, f1s\n",
    "\n",
    "def tokens_from_text(text):\n",
    "    cleaned = clean_text(text)\n",
    "    return cleaned.split()\n",
    "\n",
    "def attn_scores(model, x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, alpha = model(x)\n",
    "    return alpha.squeeze(0).squeeze(-1).cpu().numpy()\n",
    "\n",
    "def integrated_gradients_scores(model, x):\n",
    "    ig = IntegratedGradients(lambda inp: model(inp)[0])\n",
    "    baseline = x.clone()\n",
    "    baseline[:] = 0\n",
    "    attributions = ig.attribute(x, baselines=baseline, n_steps=32)\n",
    "    return attributions.abs().sum(dim=2).squeeze(0).cpu().numpy()\n",
    "\n",
    "def saliency_scores(model, x):\n",
    "    sal = Saliency(lambda inp: model(inp)[0])\n",
    "    grads = sal.attribute(x, target=None)\n",
    "    return grads.abs().sum(dim=2).squeeze(0).cpu().numpy()\n",
    "\n",
    "def deletion_insertion_curve(text, model, vocab, max_len, method=\"attention\", steps=10, mode=\"deletion\"):\n",
    "    \"\"\"Deletion/Insertion 忠实度曲线\n",
    "\n",
    "    依据显著性分数（Attention/IG/Saliency）排序逐步删除或插入词，记录概率变化序列。\"\"\"\n",
    "    tokens = tokens_from_text(text)\n",
    "    idx = text_to_indices(\" \".join(tokens), vocab, max_len).unsqueeze(0).to(device)\n",
    "    if method == \"attention\":\n",
    "        scores = attn_scores(model, idx)[:len(tokens)]\n",
    "    elif method == \"ig\":\n",
    "        scores = integrated_gradients_scores(model, idx)[:len(tokens)]\n",
    "    else:\n",
    "        scores = saliency_scores(model, idx)[:len(tokens)]\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    values = []\n",
    "    for k in range(0, min(len(tokens), steps)+1):\n",
    "        sel = order[:k]\n",
    "        mod_tokens = tokens.copy()\n",
    "        if mode == \"deletion\":\n",
    "            for p in sel:\n",
    "                mod_tokens[p] = \"\"\n",
    "        else:\n",
    "            for p in sel:\n",
    "                mod_tokens[p] = \"good\"\n",
    "        idx2 = text_to_indices(\" \".join([t for t in mod_tokens if t!=\"\"]), vocab, max_len).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out, _ = model(idx2)\n",
    "            prob = torch.sigmoid(out).item()\n",
    "        values.append(prob)\n",
    "    return np.array(values)\n",
    "\n",
    "def load_sst2():\n",
    "    ds = load_dataset(\"glue\", \"sst2\")\n",
    "    train = ds[\"train\"]\n",
    "    test = ds[\"validation\"]\n",
    "    df_train = pd.DataFrame({\"review\": train[\"sentence\"], \"sentiment\": [\"positive\" if l==1 else \"negative\" for l in train[\"label\"]]})\n",
    "    df_test = pd.DataFrame({\"review\": test[\"sentence\"], \"sentiment\": [\"positive\" if l==1 else \"negative\" for l in test[\"label\"]]})\n",
    "    return df_train, df_test\n",
    "\n",
    "def load_yelp():\n",
    "    ds = load_dataset(\"yelp_polarity\")\n",
    "    train = ds[\"train\"].shuffle(seed=42).select(range(50000))\n",
    "    test = ds[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "    df_train = pd.DataFrame({\"review\": train[\"text\"], \"sentiment\": [\"positive\" if l==1 else \"negative\" for l in train[\"label\"]]})\n",
    "    df_test = pd.DataFrame({\"review\": test[\"text\"], \"sentiment\": [\"positive\" if l==1 else \"negative\" for l in test[\"label\"]]})\n",
    "    return df_train, df_test\n",
    "\n",
    "def load_amazon():\n",
    "    ds = load_dataset(\"amazon_polarity\")\n",
    "    train = ds[\"train\"].shuffle(seed=42).select(range(50000))\n",
    "    test = ds[\"test\"].shuffle(seed=42).select(range(10000))\n",
    "    df_train = pd.DataFrame({\"review\": train[\"content\"], \"sentiment\": [\"positive\" if l==1 else \"negative\" for l in train[\"label\"]]})\n",
    "    df_test = pd.DataFrame({\"review\": test[\"content\"], \"sentiment\": [\"positive\" if l==1 else \"negative\" for l in test[\"label\"]]})\n",
    "    return df_train, df_test\n",
    "\n",
    "def df_to_current_pipeline(df):\n",
    "    d = df.copy()\n",
    "    d['cleaned_review'] = d['review'].apply(clean_text)\n",
    "    d['label'] = d['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "    return d\n",
    "\n",
    "def add_noise(text, prob=0.1):\n",
    "    tokens = tokens_from_text(text)\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for i in range(len(tokens)):\n",
    "        if np.random.rand() < prob and len(tokens[i])>0:\n",
    "            j = np.random.randint(0, len(tokens[i]))\n",
    "            tokens[i] = tokens[i][:j] + np.random.choice(list(letters)) + tokens[i][j:]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def case_punct_perturb(text, prob=0.1):\n",
    "    t = text.lower()\n",
    "    if np.random.rand() < prob:\n",
    "        t = t.upper()\n",
    "    t = re.sub(r'[^\\w\\s]', '', t)\n",
    "    return t\n",
    "\n",
    "def temperature_scale(model, loader, name=\"Model\"):\n",
    "    \"\"\"温度缩放（binary）：优化温度参数 T 以降低校准误差，返回最优 T。\"\"\"\n",
    "    T = torch.tensor([1.0], requires_grad=True, device=device)\n",
    "    optimizer = optim.LBFGS([T], lr=0.1, max_iter=50)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss_total = 0.0\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)\n",
    "            if name == \"Attention\":\n",
    "                logits, _ = model(inputs)\n",
    "            else:\n",
    "                logits = model(inputs)\n",
    "            scaled = logits / T\n",
    "            loss = criterion(scaled, labels)\n",
    "            loss.backward()\n",
    "            loss_total += loss.item()\n",
    "        return torch.tensor(loss_total, device=device)\n",
    "    optimizer.step(closure)\n",
    "    return T.detach().item()\n",
    "\n",
    "def compute_ece(probs, labels, n_bins=15):\n",
    "    \"\"\"期望校准误差（ECE）：按置信度分段计算置信度与准确率差异的加权平均。\"\"\"\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (probs >= bins[i]) & (probs < bins[i+1])\n",
    "        if mask.sum()==0:\n",
    "            continue\n",
    "        conf = probs[mask].mean()\n",
    "        acc = (labels[mask] == (probs[mask]>0.5).astype(int)).mean()\n",
    "        ece += (mask.sum()/len(probs))*abs(conf-acc)\n",
    "    return float(ece)\n",
    "\n",
    "def mc_dropout_probs(model, loader, name=\"Model\", iters=20):\n",
    "    \"\"\"MC Dropout 概率采样：启用训练态以保留 dropout，进行多次前向，返回概率矩阵。\"\"\"\n",
    "    model.train()\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iters):\n",
    "            probs_iter = []\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                if name == \"Attention\":\n",
    "                    logits, _ = model(inputs)\n",
    "                else:\n",
    "                    logits = model(inputs)\n",
    "                probs_iter.extend(torch.sigmoid(logits).squeeze(1).cpu().numpy().tolist())\n",
    "            all_probs.append(np.array(probs_iter))\n",
    "    return np.stack(all_probs, axis=0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    baseline_model = BiLSTM_Baseline(len(vocab) + 1).to(device)\n",
    "    attn_model = BiLSTM_Attention(len(vocab) + 1).to(device)\n",
    "    baseline_mean = BiLSTM_Baseline_Mean(len(vocab) + 1).to(device)\n",
    "    cnn_text = CNN_Text(len(vocab) + 1).to(device)\n",
    "    coattn = CoAttention_CNN_BiLSTM(len(vocab) + 1).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    lr = 0.001\n",
    "    epochs = 5\n",
    "\n",
    "    history = {'baseline_loss': [], 'attn_loss': [], 'baseline_acc': [], 'attn_acc': []}\n",
    "    builder_bm = lambda: BiLSTM_Baseline_Mean(len(vocab)+1)\n",
    "    builder_cnn = lambda: CNN_Text(len(vocab)+1, embed_dim=128, num_filters=128, kernel_sizes=(3,4,5,7), dropout=0.5)\n",
    "    builder_co  = lambda: CoAttention_CNN_BiLSTM(len(vocab)+1, embed_dim=128, hidden_dim=128, num_filters=128, kernel_sizes=(3,4,5,7), dropout=0.5)\n",
    "    best_lr_bm, _ = lr_grid_search(builder_bm, \"Baseline\", train_loader, val_loader, lrs=(1e-3, 5e-4, 1e-4), epochs=2)\n",
    "    best_lr_cnn, _ = lr_grid_search(builder_cnn, \"Baseline\", train_loader, val_loader, lrs=(1e-3, 5e-4, 1e-4), epochs=2)\n",
    "    best_lr_co, _  = lr_grid_search(builder_co,  \"Attention\", train_loader, val_loader, lrs=(1e-3, 5e-4, 1e-4), epochs=2)\n",
    "\n",
    "    print(\"--- Starting Training ---\")\n",
    "    for epoch in range(epochs):\n",
    "        loss_b, acc_b = train_model(\n",
    "            baseline_model, train_loader,\n",
    "            optim.Adam(baseline_model.parameters(), lr=lr),\n",
    "            criterion, \"Baseline\"\n",
    "        )\n",
    "        loss_a, acc_a = train_model(\n",
    "            attn_model, train_loader,\n",
    "            optim.Adam(attn_model.parameters(), lr=lr),\n",
    "            criterion, \"Attention\"\n",
    "        )\n",
    "        loss_bm, acc_bm = train_model(\n",
    "            baseline_mean, train_loader,\n",
    "            optim.Adam(baseline_mean.parameters(), lr=best_lr_bm if best_lr_bm is not None else lr),\n",
    "            criterion, \"Baseline\"\n",
    "        )\n",
    "        loss_c, acc_c = train_model(\n",
    "            cnn_text, train_loader,\n",
    "            optim.Adam(cnn_text.parameters(), lr=best_lr_cnn if best_lr_cnn is not None else lr),\n",
    "            criterion, \"Baseline\"\n",
    "        )\n",
    "        loss_co, acc_co = train_model(\n",
    "            coattn, train_loader,\n",
    "            optim.Adam(coattn.parameters(), lr=best_lr_co if best_lr_co is not None else lr),\n",
    "            criterion, \"Attention\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} | \"\n",
    "            f\"Base Loss: {loss_b:.4f} Acc: {acc_b:.4f} | \"\n",
    "            f\"Attn Loss: {loss_a:.4f} Acc: {acc_a:.4f} | \"\n",
    "            f\"BiLSTM-Mean Loss: {loss_bm:.4f} Acc: {acc_bm:.4f} | \"\n",
    "            f\"CNN-Text Loss: {loss_c:.4f} Acc: {acc_c:.4f} | \"\n",
    "            f\"CoAttn Loss: {loss_co:.4f} Acc: {acc_co:.4f}\"\n",
    "        )\n",
    "         \n",
    "        thr_b  = find_best_threshold(baseline_model, val_loader, \"Baseline\")\n",
    "        thr_a  = find_best_threshold(attn_model, val_loader, \"Attention\")\n",
    "        thr_bm = find_best_threshold(baseline_mean, val_loader, \"Baseline\")\n",
    "        thr_c  = find_best_threshold(cnn_text, val_loader, \"Baseline\")\n",
    "        thr_co = find_best_threshold(coattn, val_loader, \"Attention\")\n",
    "\n",
    "    test_acc_b,  test_f1_b  = evaluate_model(baseline_model, test_loader, \"Baseline\",   thr_b)\n",
    "    test_acc_a,  test_f1_a  = evaluate_model(attn_model,     test_loader, \"Attention\",  thr_a)\n",
    "    test_acc_bm, test_f1_bm = evaluate_model(baseline_mean,  test_loader, \"Baseline\",   thr_bm)\n",
    "    test_acc_c,  test_f1_c  = evaluate_model(cnn_text,       test_loader, \"Baseline\",   thr_c)\n",
    "    test_acc_co, test_f1_co = evaluate_model(coattn,         test_loader, \"Attention\",  thr_co)\n",
    "    print(f\"BiLSTM-Mean : Accuracy = {test_acc_bm:.4f}, F1-Score = {test_f1_bm:.4f}\")\n",
    "    print(f\"CNN-Text    : Accuracy = {test_acc_c:.4f}, F1-Score = {test_f1_c:.4f}\")\n",
    "    print(f\"Co-Attn     : Accuracy = {test_acc_co:.4f}, F1-Score = {test_f1_co:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Final Results (Test Set) ---\")\n",
    "    print(f\"Baseline (Bi-LSTM)     : Accuracy = {test_acc_b:.4f}, F1-Score = {test_f1_b:.4f}\")\n",
    "    print(f\"Proposed (Attn-BiLSTM) : Accuracy = {test_acc_a:.4f}, F1-Score = {test_f1_a:.4f}\")\n",
    "\n",
    "    use_gpu = (device.type == 'cuda')\n",
    "    bert_model, bert_tok, bert_acc_v, bert_f1_v, bert_params, bert_t, bert_thr = train_transformer(\n",
    "        \"bert-base-uncased\", train_df, val_df,\n",
    "        max_len=256,\n",
    "        epochs=(3 if use_gpu else 1),\n",
    "        batch_size=(32 if use_gpu else 8),\n",
    "        lr=2e-5,\n",
    "        limit_train=(None if use_gpu else 10000),\n",
    "        limit_val=(None if use_gpu else 5000),\n",
    "        max_train_batches=(None if use_gpu else 200),\n",
    "        freeze_backbone=(False if use_gpu else True)\n",
    "    )\n",
    "    roberta_model, roberta_tok, roberta_acc_v, roberta_f1_v, roberta_params, roberta_t, roberta_thr = train_transformer(\n",
    "        \"roberta-base\", train_df, val_df,\n",
    "        max_len=256,\n",
    "        epochs=(3 if use_gpu else 1),\n",
    "        batch_size=(32 if use_gpu else 8),\n",
    "        lr=2e-5,\n",
    "        limit_train=(None if use_gpu else 10000),\n",
    "        limit_val=(None if use_gpu else 5000),\n",
    "        max_train_batches=(None if use_gpu else 200),\n",
    "        freeze_backbone=(False if use_gpu else True)\n",
    "    )\n",
    "    distil_model, distil_tok, distil_acc_v, distil_f1_v, distil_params, distil_t, distil_thr = train_transformer(\n",
    "        \"distilbert-base-uncased\", train_df, val_df,\n",
    "        max_len=256,\n",
    "        epochs=(3 if use_gpu else 1),\n",
    "        batch_size=(64 if use_gpu else 16),\n",
    "        lr=3e-5,\n",
    "        limit_train=(None if use_gpu else 10000),\n",
    "        limit_val=(None if use_gpu else 5000),\n",
    "        max_train_batches=(None if use_gpu else 200),\n",
    "        freeze_backbone=(False if use_gpu else True)\n",
    "    )\n",
    "    bert_acc_t, bert_f1_t = evaluate_transformer(bert_model, bert_tok, test_df, max_len=256)\n",
    "    roberta_acc_t, roberta_f1_t = evaluate_transformer(roberta_model, roberta_tok, test_df, max_len=256)\n",
    "    distil_acc_t, distil_f1_t = evaluate_transformer(distil_model, distil_tok, test_df, max_len=256)\n",
    "    print(f\"BERT-base  : Acc={bert_acc_t:.4f}, F1={bert_f1_t:.4f}, Params={bert_params/1e6:.1f}M, TrainTime={bert_t:.1f}s, Throughput={bert_thr:.1f}/s\")\n",
    "    print(f\"RoBERTa-b  : Acc={roberta_acc_t:.4f}, F1={roberta_f1_t:.4f}, Params={roberta_params/1e6:.1f}M, TrainTime={roberta_t:.1f}s, Throughput={roberta_thr:.1f}/s\")\n",
    "    print(f\"DistilBERT : Acc={distil_acc_t:.4f}, F1={distil_f1_t:.4f}, Params={distil_params/1e6:.1f}M, TrainTime={distil_t:.1f}s, Throughput={distil_thr:.1f}/s\")\n",
    "\n",
    "\n",
    "    lstm_only = BiLSTM_Baseline_Mean(len(vocab) + 1).to(device)\n",
    "    cnn_only = CNN_Text(len(vocab) + 1).to(device)\n",
    "    fusion = CoAttention_CNN_BiLSTM(len(vocab) + 1).to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        _ = train_model(lstm_only, train_loader, optim.Adam(lstm_only.parameters(), lr=lr), criterion, \"Baseline\")\n",
    "        _ = train_model(cnn_only, train_loader, optim.Adam(cnn_only.parameters(), lr=lr), criterion, \"Baseline\")\n",
    "        _ = train_model(fusion, train_loader, optim.Adam(fusion.parameters(), lr=lr), criterion, \"Attention\")\n",
    "\n",
    "    acc_lstm, f1_lstm = evaluate_model(lstm_only, test_loader, \"Baseline\")\n",
    "    acc_cnn, f1_cnn = evaluate_model(cnn_only, test_loader, \"Baseline\")\n",
    "    acc_fus, f1_fus = evaluate_model(fusion, test_loader, \"Attention\")\n",
    "    print(f\"Ablation LSTM-only : Acc = {acc_lstm:.4f}, F1 = {f1_lstm:.4f}\")\n",
    "    print(f\"Ablation CNN-only  : Acc = {acc_cnn:.4f}, F1 = {f1_cnn:.4f}\")\n",
    "    print(f\"Ablation Fusion    : Acc = {acc_fus:.4f}, F1 = {f1_fus:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Interpretability Visualization ---\")\n",
    "    sample_text = \"This movie was absolutely terrible and a waste of time but the acting was okay\"\n",
    "    visualize_attention(sample_text, attn_model, vocab, MAX_LEN)\n",
    "    generate_attention_figures(attn_model, vocab, MAX_LEN)\n",
    "\n",
    "    save_attention_from_dataset(attn_model, test_df, vocab, MAX_LEN, n_pos=3, n_neg=3)\n",
    "    save_length_bucket_examples(attn_model, test_df, vocab, MAX_LEN)\n",
    "    save_dataset_curves(attn_model, test_df, \"Attention\", vocab, MAX_LEN)\n",
    "    save_threshold_sweep(attn_model, val_df, \"Attention\", vocab, MAX_LEN)\n",
    "    accs_a, f1s_a = run_multi_seed(lambda: BiLSTM_Attention(len(vocab)+1), \"Attention\", train_loader, test_loader, seeds=(42,43,44,45,46))\n",
    "    accs_b, f1s_b = run_multi_seed(lambda: BiLSTM_Baseline(len(vocab)+1), \"Baseline\", train_loader, test_loader, seeds=(42,43,44,45,46))\n",
    "    m_a, s_a, lo_a, hi_a = bootstrap_ci(f1s_a, n_boot=1000)\n",
    "    m_b, s_b, lo_b, hi_b = bootstrap_ci(f1s_b, n_boot=1000)\n",
    "    t_stat, p_val = ttest_rel(f1s_a, f1s_b)\n",
    "    print(f\"Attn-BiLSTM F1 mean={m_a:.4f} std={s_a:.4f} 95%CI=[{lo_a:.4f},{hi_a:.4f}] vs Baseline mean={m_b:.4f} std={s_b:.4f} 95%CI=[{lo_b:.4f},{hi_b:.4f}] paired_t p={p_val:.4f}\")\n",
    "    curve_del = deletion_insertion_curve(\"this movie was absolutely terrible and a waste of time\", attn_model, vocab, MAX_LEN, method=\"attention\", steps=20, mode=\"deletion\")\n",
    "    curve_ins = deletion_insertion_curve(\"i loved the story it was amazing and touching\", attn_model, vocab, MAX_LEN, method=\"attention\", steps=20, mode=\"insertion\")\n",
    "    os.makedirs('figs', exist_ok=True)\n",
    "    plt.figure(); plt.plot(curve_del); plt.title('Deletion'); plt.savefig(os.path.join('figs','deletion_curve.png'), bbox_inches='tight', dpi=200); plt.close()\n",
    "    plt.figure(); plt.plot(curve_ins); plt.title('Insertion'); plt.savefig(os.path.join('figs','insertion_curve.png'), bbox_inches='tight', dpi=200); plt.close()\n",
    "    sample = \"the story was amazing and the acting superb\"\n",
    "    noisy = add_noise(sample, prob=0.2)\n",
    "    pert = case_punct_perturb(sample, prob=1.0)\n",
    "    visualize_attention(noisy, attn_model, vocab, MAX_LEN, os.path.join('figs','attn_noisy.png'))\n",
    "    visualize_attention(pert, attn_model, vocab, MAX_LEN, os.path.join('figs','attn_perturbed.png'))\n",
    "    T_opt = temperature_scale(attn_model, val_loader, \"Attention\")\n",
    "    probs = []\n",
    "    labs = []\n",
    "    with torch.no_grad():\n",
    "        for inp, y in test_loader:\n",
    "            inp = inp.to(device)\n",
    "            logits, _ = attn_model(inp)\n",
    "            scaled = logits / T_opt\n",
    "            p = torch.sigmoid(scaled).squeeze(1).cpu().numpy()\n",
    "            probs.extend(p); labs.extend(y.numpy())\n",
    "    probs = np.array(probs); labs = np.array(labs).astype(int)\n",
    "    ece = compute_ece(probs, labs, n_bins=15)\n",
    "    print(f\"ECE(calibrated)={ece:.4f}\")\n",
    "    plt.figure()\n",
    "    bins = np.linspace(0.0,1.0,16)\n",
    "    bin_centers = 0.5*(bins[:-1]+bins[1:])\n",
    "    accs = []\n",
    "    confs = []\n",
    "    for i in range(15):\n",
    "        m = (probs>=bins[i]) & (probs<bins[i+1])\n",
    "        if m.sum()==0:\n",
    "            confs.append(0.0); accs.append(0.0)\n",
    "        else:\n",
    "            confs.append(probs[m].mean()); accs.append((labs[m]==(probs[m]>0.5).astype(int)).mean())\n",
    "    plt.plot(bin_centers, accs, label='Accuracy')\n",
    "    plt.plot(bin_centers, confs, label='Confidence')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join('figs','reliability_calibrated.png'), bbox_inches='tight', dpi=200)\n",
    "    plt.close()\n",
    "    mc = mc_dropout_probs(coattn, test_loader, \"Attention\", iters=20)\n",
    "    unc = mc.std(axis=0)\n",
    "    np.save(os.path.join('figs','mc_uncertainty.npy'), unc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
